---
title: "Pilot Analysis"
author: "Howard Baek"
date: "6/17/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries and Helper Files
```{r}
library(tidyverse)
library(lubridate)
library(scales)
library(factoextra)
library(raster)
library(tmap)
library(png)
library(gridExtra)
library(dendextend)
library(DescTools)
library(imageryML)
library(plotly)
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
theme_set(theme_light())

# Note: Don't need to source these anymore since we have the imageryML package
# source(file.path(here::here(), "R", "imgVectortoRaster.R"))
# source(file.path(here::here(), "R", "dendIMG.R"))
# source(file.path(here::here(), "R", "myheatmap.R"))
# source(file.path(here::here(), "R", "desat.R"))
# source(file.path(here::here(), "R", "kheatmap.R"))
# source(file.path(here::here(), "R", "addIMGtopanel.R"))
# source(file.path(here::here(), "R", "yearTable.R"))
# Code to download data from ERDDAP servers is in data folder
```


## Re-run Eli talk analyses 

Latitude (42.625, 52.125) and Longitude (229.875, 236.625)

## Data Preprocessing
```{r}
# I downloaded the data using read_csv("https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.csv?sst%5B(2010-01-01T12:00:00Z):(2021-06-01T12:00:00Z)%5D%5B(0.0)%5D%5B(42.625):(52.125)%5D%5B(229.875):(236.625)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C12%7C19%7C&.bgColor=0xffccccff")
# This big data is saved as pilot_data.csv

# Raw data (last 10 years)
df_raw <- read_csv("howard-analysis/pilot_data.csv")

# Processed data
df_processed <- df_raw %>% 
  # Get rid of miscellaneous zlev in first row
  slice(-1) %>% 
  # zlev is a column of zeroes, so get rid of that
  dplyr::select(-zlev) %>% 
  # Convert into date
  mutate(time = ymd_hms(time),
         # Extract out day so I can just filter for first day in each month
         day = day(time)) %>% 
  # filter for first day in each month
  filter(day == 1) %>% 
  dplyr::select(-day) %>% 
  # Set column names
  rename(date = time,
         lat = latitude,
         lon = longitude) %>% 
  # Convert date column to Date type
  mutate(date = as.Date(date),
         lat = as.numeric(lat),
         lon = as.numeric(lon),
         sst = as.numeric(sst))

# mask out Puget Sound, Strait of Juan de Fuca and Georgia Strait
masks <- list(c(235.4488, 236.884, 47.87651, 50.13138),
              c(232.2913, 233.8987, 50.28689, 51.60871),
              c(234.4154, 235.9654, 49.04283, 50.09251))

for(m1 in masks) {
  # index of Puget Sound, Strait of Juan de Fuca or Georgia Strait
  mask_loc <- df_processed$lat <= m1[4] & df_processed$lat >= m1[3] &
    df_processed$lon <= m1[2] & df_processed$lon >= m1[1]
  # Change to NA
  df_processed$sst[mask_loc] <- NA
}


# Use this code to figure out the lat/lon extents
# a <- imgVectortoRaster(datalist$dat.clean[3:5,], datalist)
# raster::plot(a$stack)
# drawExtent() # click in upper/lower corners
```

```{r}
# Check if same n for each date
n.by.date <- tapply(df_processed$sst, df_processed$date, function(x){sum(is.na(x))})
if(any((n.by.date-n.by.date[1])!=0)) {
  stop("There's a problem. Should be same n for each date.")
} 

# Pivot Wider
df_wide <- df_processed %>% 
  pivot_wider(names_from = date, values_from = sst)

# which row are non-NA?
pos.loc <- which(!is.na(df_wide[,3]))

# Omit data and take transpose
df.clean <- df_wide %>% 
  # remove the rows that are NA
  na.omit()

# Take transpose of df_wide and df.clean to mimic output of processCSV-tidyverse.R
df_wide_t <- t(df_wide)
df.clean.t <- t(df.clean)

datalist <- list(dat = df_wide_t, dat.clean = df.clean.t, pos.loc = pos.loc)
```


```{r}
# Take out rows with lat, lon
image <- df.clean.t[c(-1,-2),]
# image %>% View()

# Create image_norm (X_norm in QuanSeminar.Rmd)
image_norm <- t(scale(t(image), scale=FALSE))
colnames(image_norm) <- paste0("p", 1:ncol(image_norm))

# image_norm %>% View()
```


Note: image_norm is a 138 by 805 matrix (Context: 138 images on 138 different dates, first day of each month from the past 10 years, with 805 pixels)

```{r}
round(image_norm[1:5, 1:10], digits=2)
```


## Principal Component Analysis

```{r}
# Apply PCA
prcomp_pca <- prcomp(image_norm, scale = FALSE, center=FALSE)

# Do this so the first eigenimage looks like upwelling
# each column in prcomp_pca$rotation is an "eigenimage"
prcomp_pca$rotation[,1] <- -1 * prcomp_pca$rotation[,1]
prcomp_pca$x[,1] <- -1 * prcomp_pca$x[,1]

# Store for use later
# eigenimages is the transpose of the rotation matrix, which provides 
# the principal component loadings: each column of the rotation matrix
# provides the corresponding principal component loading vector (ISL pg 402)
eigenimages <- t(prcomp_pca$rotation)
# alpha's kth column is the kth principal component score vector (z)
alpha <- prcomp_pca$x
```

`eigenimages` is a 138 by 805 matrix, where where are 138 PCs (eigenimages) and 776 pixels (p1, p2, p3, ..., p776)


```{r}
df <- data.frame(alpha,
                date = as.Date(rownames(image_norm)),
                year=as.integer(format(as.Date(rownames(image_norm)), "%Y")),
                mon=factor(format(as.Date(rownames(image_norm)), "%b"), levels = month.abb))
# Get rid of rownames
rownames(df) <- NULL
# Make date_label for graph
df <- df %>% 
  mutate(date_label = paste(mon, year, sep = ","))

df2 <- df %>% 
  pivot_longer(starts_with("PC"), names_to="PC", values_to="value")
```



### PC1 and PC2 Scores
```{r}
# Visualize PCs
# Recreate Eli's graph: SST Anomaly pattern in the PC1-PC2 space
df %>% 
  ggplot(aes(PC1, PC2, label = date_label)) +
  geom_point(alpha = 0.3) +
  geom_text(check_overlap = TRUE, vjust = 1, hjust = 1) +
  annotate("segment", x = -5, xend = 55, y = 0, yend = 0, colour = "midnightblue", linetype=2) +
  annotate("segment", x = 0, xend = 0, y = -25, yend = 25, colour = "midnightblue", linetype=2) +
  ggtitle("PCA clusters months together",
          subtitle = "April&May&June and Feb&March and July&August and Sept&Oct") 
```

### PCA: Facet by month

#### Static
```{r}
df %>% 
  ggplot(aes(PC1, PC2, label = year)) +
  geom_point(alpha = 0.8) +
  geom_text(check_overlap = TRUE, vjust = 1, hjust = 1) +
  facet_wrap(~mon) +
  annotate("segment", x = -5, xend = 55, y = 0, yend = 0, colour = "midnightblue", linetype=2) +
  annotate("segment", x = 0, xend = 0, y = -25, yend = 25, colour = "midnightblue", linetype=2) +
  ggtitle("PCA decomposition into PC1 Score and PC2 Score by Month",
          subtitle = "Shows seasonal cycle: Parabolic Movement from Jan to Dec") 
```


#### Interactive
```{r}
pca_by_month_plot <- df %>% 
  ggplot(aes(PC1, PC2, label = year)) +
  geom_point(alpha = 0.7) +
  facet_wrap(~mon) +
  annotate("segment", x = -5, xend = 55, y = 0, yend = 0, colour = "midnightblue", linetype=2) +
  annotate("segment", x = 0, xend = 0, y = -25, yend = 25, colour = "midnightblue", linetype=2) +
  ggtitle("PCA decomposition into PC1 Score and PC2 Score by Month",
          subtitle = "Shows seasonal cycle: Parabolic Movement from Jan to Dec") 

ggplotly(pca_by_month_plot)
```


Jul: 2015, 2016 seems like outlier
Sept: 2017, 2020 seems like an outlier


### PCA: Facet by Year

##### Static
```{r}
df %>%
  ggplot(aes(PC1, PC2, label = mon, vjust = 1, hjust = 1)) +
  geom_point(alpha = 0.8) +
  geom_text(check_overlap = TRUE) +
  facet_wrap(~year) +
  annotate("segment", x = -5, xend = 55, y = 0, yend = 0, colour = "midnightblue", linetype=2) +
  annotate("segment", x = 0, xend = 0, y = -25, yend = 25, colour = "midnightblue", linetype=2) +
   ggtitle("PCA decomposition into PC1 Score and PC2 Score by Year",
          subtitle = "Shows seasonal cycle, missing summer months for 2021") 
```


#### Interactive
```{r}
pca_by_year_plot <- df %>%
  ggplot(aes(PC1, PC2, label = mon)) +
  geom_point(alpha = 0.8) +
  facet_wrap(~year) +
  annotate("segment", x = -5, xend = 55, y = 0, yend = 0, colour = "midnightblue", linetype=2) +
  annotate("segment", x = 0, xend = 0, y = -25, yend = 25, colour = "midnightblue", linetype=2) +
   ggtitle("PCA decomposition into PC1 Score and PC2 Score by Year",
          subtitle = "Shows seasonal cycle, missing summer months for 2021") 

ggplotly(pca_by_year_plot)
```


2020: September an outlier





### Vector to Raster (img.list)
```{r}
# TODO: proj.name=NULL from imageryML R package
img.list <- imgVectortoRaster(eigenimages, datalist)$list
```




### PCA: 6 Eigen Images
```{r}
# First 6 eigenimages
p_stack <- raster::stack(img.list[[1]], img.list[[2]], img.list[[3]], 
                 img.list[[4]], img.list[[5]], img.list[[6]])
pal <- colorRamps::matlab.like(100)

# crs(p_stack) <- "+proj=longlat +datum=WGS84"

tm_shape(p_stack) +
  tm_raster(style= "cont", title="SST Anomaly", 
            palette=pal, midpoint=NA, 
            colorNA = "grey", textNA = "Land") +
  tm_layout(panel.labels = paste("PC", 1:length(p_stack))) +
  tm_layout(main.title = "Eigen Images", title.size = 1)
```


```{r}
# First PC
ncomp1 <- 1
# alpha[,1:ncomp1, drop=FALSE] means select the first ncomp1 column (Principal Component)
RE1 <- alpha[,1:ncomp1, drop=FALSE] %*% eigenimages[1:ncomp1, drop=FALSE]

# First 2 PCs
ncomp2 <- 2
RE2 <- alpha[,1:ncomp2] %*% eigenimages[1:ncomp2,]

# First 10 PCs
ncomp3 <- 10
RE3 <- alpha[,1:ncomp3] %*% eigenimages[1:ncomp3,]
```


### Reconstruction of Images with PCA 

2015-10-01
```{r}
# Index for date 2015-10-01
i <- which(rownames(image_norm) =="2015-10-01")

img1 <- imgVectortoRaster(rbind(image_norm[i,], RE1[i,], RE2[i,], RE3[i,]), datalist)$stack

tm_shape(img1) + 
  tm_raster(style= "cont", title="SST Anomaly", 
            palette=pal, midpoint=NA, 
            colorNA = "grey", textNA = "Land") +
  tm_layout(panel.labels = c("True", paste(c(ncomp1, ncomp2, ncomp3),"PC")),
            title=rownames(image_norm)[i])
```


2020-06-01
```{r}
# Index for date 2020-06-01
i <- which(rownames(image_norm) =="2020-06-01")

img1 <- imgVectortoRaster(rbind(image_norm[i,], RE1[i,], RE2[i,], RE3[i,]), datalist)$stack

tm_shape(img1) + 
  tm_raster(style= "cont", title="SST Anomaly", 
            palette=pal, midpoint=NA, 
            colorNA = "grey", textNA = "Land") +
  tm_layout(panel.labels = c("True", paste(c(ncomp1, ncomp2, ncomp3),"PC")),
            title=rownames(image_norm)[i])
```



## K-Means Clustering

Resource: https://uc-r.github.io/kmeans_clustering

```{r}
set.seed(123)

image_norm_kmeans <- t(scale(t(image)))
colnames(image_norm_kmeans) <- paste0("p", 1:ncol(image_norm_kmeans))

# Number of Clusters
n_K <- 3
# Run K-Means
out_norm <- kmeans(image_norm_kmeans, n_K, iter.max=25, nstart=100)

centroidimages_norm <- out_norm$centers
rownames(centroidimages_norm) <- paste("Centroid", 1:n_K)

# Convert to Raster
img_norm <- imgVectortoRaster(centroidimages_norm, datalist)
img_norm_stack <- img_norm$stack
img_norm_list <- img_norm$list
```

```{r}
# Have a look at centroids
round(centroidimages_norm[1:3, 1:10], digits=2)
```


### Centroid Images
```{r}
tm_shape(img_norm_stack) + 
  tm_raster(style= "cont", title="SST Anomaly", 
            palette=pal, midpoint=NA, 
            colorNA = "grey", textNA = "Land") +
  tm_layout(panel.labels = paste("Centroid", 1:length(p_stack))) +
  tm_layout(main.title = "Centroid Images", title.size = 1)
```


### Clustering Visualization
```{r}
# fviz_cluster:
# Provides ggplot2-based elegant visualization of partitioning methods including kmeans Observations are represented by points in the plot, using principal components if ncol(data) # is larger than 2. An ellipse is drawn around each cluster.
fviz_cluster(out_norm, data = image_norm_kmeans, ggtheme = theme_light())
```


### Distance Visualization
```{r}
# First 12 months (2010)
distance_matrix <- get_dist(image_norm_kmeans[127:138, ])
fviz_dist(distance_matrix, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

March-May (Spring-ish) are the most different from August-October (Late Summer - Fall-ish).


### Determine Optimal Clusters

#### Elbow Method
```{r}
# function to compute total within-cluster sum of square 
wss <- function(n_K) {
  kmeans(image_norm_kmeans, n_K, iter.max=25, nstart=100)$tot.withinss
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

wss_df <- tibble(k.values, wss_values)

wss_df %>% 
  ggplot(aes(k.values, wss_values)) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "# of Clusters",
       y = "Total Within-Cluster Sum of Square") +
  ggtitle("Elbow Method: 5, 6 or 7 clusters seems to be the optimal number")
```


#### Silhouette Method
```{r}
# function to compute average silhouette for k clusters
avg_sil <- function(n_K) {
  km.res <- kmeans(image_norm_kmeans, n_K, iter.max=25, nstart=100)
  ss <- silhouette(km.res$cluster, dist(image_norm_kmeans))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

# Create df of silhouette method
sil_df <- tibble(k.values, avg_sil_values)

sil_df %>% 
  ggplot(aes(k.values, avg_sil_values)) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "# of Clusters",
       y = "Total Within-Cluster Sum of Square") +
  ggtitle("Silhouette Method: 2 or 5 clusters seems to be the optimal number")
```


Let's roll with k = 5

### Centroid Images: K = 5
```{r}
# Compute k-means clustering with k = 5
set.seed(123)

# Number of Clusters
n_K <- 5
# Run K-Means
out_norm <- kmeans(image_norm_kmeans, n_K, iter.max=25, nstart=100)

centroidimages_norm <- out_norm$centers
rownames(centroidimages_norm) <- paste("Centroid", 1:n_K)

# Convert to Raster
img_norm <- imgVectortoRaster(centroidimages_norm, datalist)
img_norm_stack <- img_norm$stack
img_norm_list <- img_norm$list
```

```{r}
tm_shape(img_norm_stack) + 
  tm_raster(style= "cont", title="SST Anomaly", 
            palette=pal, midpoint=NA, 
            colorNA = "grey", textNA = "Land") +
  tm_layout(panel.labels = paste("Centroid", 1:length(p_stack))) +
  tm_layout(main.title = "Centroid Images", title.size = 1)
```